{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "994e737f",
   "metadata": {},
   "source": [
    "# Document Processing with Docling and Propositional Chunking\n",
    "\n",
    "This notebook demonstrates an advanced approach for processing documents into atomic, self-contained propositions using:\n",
    "1. **Docling**: Convert PDF/documents to clean markdown format\n",
    "2. **Propositional Chunking with Claude 3.5 Haiku**: Extract atomic facts and claims that can stand alone\n",
    "\n",
    "## Part 1: Document Processing with Docling\n",
    "\n",
    "### Step 1: Convert PDF to Markdown using Docling\n",
    "\n",
    "[Docling](https://github.com/DS4SD/docling) is a powerful document conversion library that can parse complex document layouts (PDFs, PowerPoints, Word docs, etc.) and convert them into clean markdown format. This is particularly useful for:\n",
    "- Preserving document structure (headings, lists, tables)\n",
    "- Handling multi-column layouts\n",
    "- Extracting text from complex academic papers\n",
    "\n",
    "In this example, we're converting a research paper from arXiv about GraphRAG (Graph-based Retrieval Augmented Generation), which we'll then process using propositional chunking for maximum retrieval precision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1958ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "source = \"https://arxiv.org/pdf/2404.16130\"\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(source)\n",
    "print(result.document.export_to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ded2dc",
   "metadata": {},
   "source": [
    "### Step 2: Install Required Dependencies\n",
    "\n",
    "For propositional chunking, we'll use:\n",
    "- **langchain-aws**: Provides integration with AWS Bedrock for both LLMs and embeddings\n",
    "- **boto3**: AWS SDK for Python to interact with AWS services\n",
    "- **langchain**: Core LangChain library for document handling\n",
    "- **typing**: For type hints and annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe22a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet langchain langchain-aws boto3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb7242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Credentials Configuration (optional)\n",
    "# If you haven't configured AWS credentials, you can do it in one of these ways:\n",
    "\n",
    "# Option 1: Use environment variables (uncomment if needed)\n",
    "# import os\n",
    "# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"your-access-key-id\"\n",
    "# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"your-secret-access-key\"\n",
    "# os.environ[\"AWS_SESSION_TOKEN\"] = \"your-session-token\"  # Optional, for temporary credentials\n",
    "\n",
    "# Option 2: Pass credentials directly to BedrockEmbeddings (uncomment if needed)\n",
    "# from langchain_aws import BedrockEmbeddings\n",
    "# bedrock_embeddings = BedrockEmbeddings(\n",
    "#     model_id=\"amazon.titan-embed-text-v2:0\",\n",
    "#     region_name=\"us-east-1\",\n",
    "#     aws_access_key_id=\"your-access-key-id\",\n",
    "#     aws_secret_access_key=\"your-secret-access-key\",\n",
    "#     aws_session_token=\"your-session-token\"  # Optional\n",
    "# )\n",
    "\n",
    "# Option 3: Use AWS CLI configuration (run in terminal)\n",
    "# aws configure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd706add",
   "metadata": {},
   "outputs": [],
   "source": [
    "### AWS Credentials Setup\n",
    "\n",
    "Make sure you have AWS credentials configured to use Bedrock. You can configure them using:\n",
    "1. AWS CLI: `aws configure`\n",
    "2. Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "3. IAM roles (if running on AWS infrastructure)\n",
    "\n",
    "The cell above shows different configuration options you can uncomment if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d698e",
   "metadata": {},
   "source": [
    "### Step 3: Configure Propositional Chunker with Claude 3.5 Haiku\n",
    "\n",
    "**Propositional Chunking** is an advanced text processing technique that:\n",
    "- Breaks down text into atomic propositions (single facts or claims)\n",
    "- Each proposition is self-contained and can stand alone\n",
    "- Maintains complete context within each proposition\n",
    "- Provides more granular and precise retrieval for RAG applications\n",
    "\n",
    "We're using **Anthropic's Claude 3.5 Haiku** model via AWS Bedrock which:\n",
    "- Offers excellent performance for text analysis tasks\n",
    "- Provides fast inference with high quality outputs\n",
    "- Is cost-effective for processing large documents\n",
    "- Excels at extracting structured information from unstructured text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b1d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import re\n",
    "\n",
    "class PropositionalChunker:\n",
    "    \"\"\"\n",
    "    A custom chunker that breaks down text into atomic propositions using Claude 3.5 Haiku.\n",
    "    Each proposition contains a single fact or claim that can stand alone.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        \n",
    "    def extract_propositions(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract atomic propositions from text using Claude 3.5 Haiku.\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"Break down the following text into atomic propositions. Each proposition should:\n",
    "1. Contain ONLY ONE fact, claim, or piece of information\n",
    "2. Be self-contained and understandable without additional context\n",
    "3. Include necessary context (e.g., who, what, when, where) to stand alone\n",
    "4. Be a complete sentence\n",
    "\n",
    "Return the propositions as a JSON array of strings.\n",
    "\n",
    "Text to analyze:\n",
    "{text}\n",
    "\n",
    "Output format:\n",
    "[\"proposition 1\", \"proposition 2\", ...]\n",
    "\n",
    "Important: Return ONLY the JSON array, no explanations or additional text.\"\"\"\n",
    "\n",
    "        response = self.llm.invoke(prompt)\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        content = response.content\n",
    "        \n",
    "        # Try to find JSON array in the response\n",
    "        json_match = re.search(r'\\[.*\\]', content, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                propositions = json.loads(json_match.group())\n",
    "                return propositions\n",
    "            except json.JSONDecodeError:\n",
    "                # Fallback to simple sentence splitting if JSON parsing fails\n",
    "                return [s.strip() for s in text.split('.') if s.strip()]\n",
    "        else:\n",
    "            # Fallback to simple sentence splitting\n",
    "            return [s.strip() for s in text.split('.') if s.strip()]\n",
    "    \n",
    "    def chunk_document(self, text: str, chunk_size: int = 5) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Convert text into document chunks, where each chunk contains \n",
    "        a group of related propositions.\n",
    "        \n",
    "        Args:\n",
    "            text: The text to chunk\n",
    "            chunk_size: Number of propositions per chunk (default: 5)\n",
    "        \n",
    "        Returns:\n",
    "            List of Document objects containing propositional chunks\n",
    "        \"\"\"\n",
    "        # Extract all propositions\n",
    "        propositions = self.extract_propositions(text)\n",
    "        \n",
    "        # Group propositions into chunks\n",
    "        chunks = []\n",
    "        for i in range(0, len(propositions), chunk_size):\n",
    "            chunk_propositions = propositions[i:i + chunk_size]\n",
    "            chunk_text = \" \".join(chunk_propositions)\n",
    "            \n",
    "            # Create metadata for the chunk\n",
    "            metadata = {\n",
    "                \"chunk_index\": i // chunk_size,\n",
    "                \"num_propositions\": len(chunk_propositions),\n",
    "                \"start_proposition\": i,\n",
    "                \"end_proposition\": min(i + chunk_size, len(propositions))\n",
    "            }\n",
    "            \n",
    "            chunks.append(Document(page_content=chunk_text, metadata=metadata))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Initialize Claude 3.5 Haiku via AWS Bedrock\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"amazon.nova-lite-v1:0\",\n",
    "    region_name=\"us-east-1\",  # Change to your preferred AWS region\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.1,  # Lower temperature for more consistent extraction\n",
    "        \"max_tokens\": 4096\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the propositional chunker\n",
    "propositional_chunker = PropositionalChunker(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bb4740",
   "metadata": {},
   "source": [
    "### Step 4: Create Propositional Document Chunks\n",
    "\n",
    "Now we'll process the markdown text through the PropositionalChunker to create document chunks. This process:\n",
    "1. Takes the markdown output from Docling\n",
    "2. Uses Claude 3.5 Haiku to extract atomic propositions (single facts/claims)\n",
    "3. Each proposition is self-contained with necessary context\n",
    "4. Groups propositions into chunks for efficient storage and retrieval\n",
    "\n",
    "This approach is superior to both naive and semantic splitting because:\n",
    "- Each proposition is a complete, standalone fact\n",
    "- No information is lost due to arbitrary boundaries\n",
    "- Provides maximum granularity for precise retrieval\n",
    "- Improves accuracy in RAG applications by avoiding partial or incomplete information\n",
    "- Better handling of complex documents with multiple interrelated topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccad6376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the document with propositional chunking\n",
    "# Note: For large documents, we'll process in smaller sections to avoid token limits\n",
    "markdown_text = result.document.export_to_markdown()\n",
    "\n",
    "# Split into manageable sections (approximately 20000 characters each)\n",
    "section_size = 20000\n",
    "sections = [markdown_text[i:i+section_size] for i in range(0, len(markdown_text), section_size)]\n",
    "\n",
    "# Process each section and collect all chunks\n",
    "all_chunks = []\n",
    "for i, section in enumerate(sections[:5]):  # Process first 5 sections for demo\n",
    "    print(f\"Processing section {i+1}/{min(5, len(sections))}...\")\n",
    "    chunks = propositional_chunker.chunk_document(section)  # Default 5 propositions per chunk\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "docs = all_chunks\n",
    "print(f\"\\nTotal chunks created: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f4ce55",
   "metadata": {},
   "source": [
    "### Step 5: Examine the Results\n",
    "\n",
    "Let's look at the generated propositional chunks to see how the PropositionalChunker has broken down the text into atomic facts. Each chunk contains a small group of self-contained propositions that can stand alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5761c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display examples of propositional chunks\n",
    "print(\"=\"*80)\n",
    "print(\"PROPOSITIONAL CHUNKING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show first few chunks with metadata\n",
    "for i, doc in enumerate(docs[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"\\nContent:\\n{doc.page_content}\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "# Show statistics\n",
    "print(f\"\\n\\nSTATISTICS:\")\n",
    "print(f\"Total chunks created: {len(docs)}\")\n",
    "print(f\"Average chunk length: {sum(len(d.page_content) for d in docs) / len(docs):.0f} characters\")\n",
    "print(f\"Min chunk length: {min(len(d.page_content) for d in docs)} characters\")\n",
    "print(f\"Max chunk length: {max(len(d.page_content) for d in docs)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef09dd06",
   "metadata": {},
   "source": [
    "## Part 2: Comparison - Propositional vs Semantic Chunking\n",
    "\n",
    "### Why Propositional Chunking?\n",
    "\n",
    "Propositional chunking offers several advantages over traditional semantic chunking:\n",
    "\n",
    "#### 1. **Atomic Information Units**\n",
    "- Each proposition contains exactly one fact or claim\n",
    "- No partial information or incomplete thoughts\n",
    "- Perfect for fact-checking and verification systems\n",
    "\n",
    "#### 2. **Self-Contained Context**\n",
    "- Every proposition includes necessary context (who, what, when, where)\n",
    "- Can be understood without referring to surrounding text\n",
    "- Ideal for question-answering systems\n",
    "\n",
    "#### 3. **Improved Retrieval Precision**\n",
    "- More granular matching with user queries\n",
    "- Reduces irrelevant information in retrieved chunks\n",
    "- Better alignment with specific information needs\n",
    "\n",
    "#### 4. **Enhanced Flexibility**\n",
    "- Propositions can be dynamically grouped based on query requirements\n",
    "- Supports both fine-grained and coarse-grained retrieval\n",
    "- Can be re-combined in different ways for different use cases\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "Propositional chunking is particularly valuable for:\n",
    "- **Legal documents**: Where every claim must be precise and traceable\n",
    "- **Scientific papers**: Where individual findings and claims need to be isolated\n",
    "- **Knowledge bases**: Where facts need to be stored and retrieved independently\n",
    "- **Fact-checking systems**: Where individual claims need verification\n",
    "- **Multi-hop reasoning**: Where complex queries require combining multiple facts\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "While propositional chunking offers superior precision, consider:\n",
    "- **Higher computational cost**: Requires LLM inference for chunking\n",
    "- **More storage**: Generally creates more chunks than semantic chunking\n",
    "- **Processing time**: Slower than embedding-based semantic chunking\n",
    "- **Token usage**: Consumes API tokens for the chunking process itself\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Adjust chunk size based on use case**: \n",
    "   - Smaller chunks (1-3 propositions) for high precision\n",
    "   - Larger chunks (5-10 propositions) for context preservation\n",
    "\n",
    "2. **Implement caching**: Store processed propositions to avoid re-processing\n",
    "\n",
    "3. **Combine with embeddings**: Use embeddings on the propositions for similarity search\n",
    "\n",
    "4. **Consider hybrid approaches**: Use propositional chunking for critical sections and semantic chunking for less critical content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1567905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Process larger portions of the document\n",
    "# Uncomment below to process the entire document (this will use more API calls)\n",
    "\n",
    "# def process_full_document(text, chunker, section_size=2000, max_sections=None):\n",
    "#     \"\"\"Process a full document with propositional chunking.\"\"\"\n",
    "#     sections = [text[i:i+section_size] for i in range(0, len(text), section_size)]\n",
    "#     \n",
    "#     if max_sections:\n",
    "#         sections = sections[:max_sections]\n",
    "#     \n",
    "#     all_chunks = []\n",
    "#     total_sections = len(sections)\n",
    "#     \n",
    "#     for i, section in enumerate(sections):\n",
    "#         print(f\"Processing section {i+1}/{total_sections}...\", end=\"\\r\")\n",
    "#         try:\n",
    "#             chunks = chunker.chunk_document(section, chunk_size=3)\n",
    "#             all_chunks.extend(chunks)\n",
    "#         except Exception as e:\n",
    "#             print(f\"\\nError processing section {i+1}: {e}\")\n",
    "#             continue\n",
    "#     \n",
    "#     print(f\"\\nCompleted! Total chunks: {len(all_chunks)}\")\n",
    "#     return all_chunks\n",
    "\n",
    "# # Process more of the document (e.g., first 20 sections)\n",
    "# full_docs = process_full_document(\n",
    "#     result.document.export_to_markdown(), \n",
    "#     propositional_chunker,\n",
    "#     max_sections=20\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de1fba",
   "metadata": {},
   "source": [
    "## Part 3: Using Propositional Chunks in RAG Systems\n",
    "\n",
    "### Integration with Vector Databases\n",
    "\n",
    "Once you have your propositional chunks, you can easily integrate them with vector databases for RAG applications:\n",
    "\n",
    "```python\n",
    "# Example: Using with ChromaDB or similar vector stores\n",
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain_aws import BedrockEmbeddings\n",
    "\n",
    "# # Initialize embeddings (can still use Titan for embedding the propositions)\n",
    "# embeddings = BedrockEmbeddings(\n",
    "#     model_id=\"amazon.titan-embed-text-v2:0\",\n",
    "#     region_name=\"us-east-1\"\n",
    "# )\n",
    "\n",
    "# # Create vector store from propositional chunks\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents=docs,\n",
    "#     embedding=embeddings,\n",
    "#     collection_name=\"propositional_chunks\"\n",
    "# )\n",
    "\n",
    "# # Query the vector store\n",
    "# query = \"What is GraphRAG?\"\n",
    "# results = vectorstore.similarity_search(query, k=5)\n",
    "```\n",
    "\n",
    "### Advanced Retrieval Strategies\n",
    "\n",
    "With propositional chunks, you can implement sophisticated retrieval strategies:\n",
    "\n",
    "1. **Multi-proposition retrieval**: Retrieve multiple related propositions and combine them\n",
    "2. **Contextual expansion**: After retrieving a proposition, fetch neighboring propositions\n",
    "3. **Hierarchical retrieval**: Store propositions at different granularity levels\n",
    "4. **Semantic re-ranking**: Use Claude to re-rank retrieved propositions based on relevance\n",
    "\n",
    "### Performance Optimization Tips\n",
    "\n",
    "- **Batch processing**: Process multiple sections in parallel when possible\n",
    "- **Caching layer**: Implement Redis or similar for caching processed propositions\n",
    "- **Incremental processing**: Process new documents incrementally rather than re-processing everything\n",
    "- **Hybrid storage**: Use both vector and graph databases for different query types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51193d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save propositional chunks for later use\n",
    "import json\n",
    "\n",
    "def save_chunks(chunks, filename=\"propositional_chunks.json\"):\n",
    "    \"\"\"Save propositional chunks to a JSON file.\"\"\"\n",
    "    chunks_data = []\n",
    "    for chunk in chunks:\n",
    "        chunks_data.append({\n",
    "            \"content\": chunk.page_content,\n",
    "            \"metadata\": chunk.metadata\n",
    "        })\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(chunks_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved {len(chunks)} chunks to {filename}\")\n",
    "\n",
    "def load_chunks(filename=\"propositional_chunks.json\"):\n",
    "    \"\"\"Load propositional chunks from a JSON file.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        chunks_data = json.load(f)\n",
    "    \n",
    "    chunks = []\n",
    "    for data in chunks_data:\n",
    "        chunks.append(Document(\n",
    "            page_content=data[\"content\"],\n",
    "            metadata=data[\"metadata\"]\n",
    "        ))\n",
    "    \n",
    "    print(f\"Loaded {len(chunks)} chunks from {filename}\")\n",
    "    return chunks\n",
    "\n",
    "# Save the chunks\n",
    "save_chunks(docs)\n",
    "\n",
    "# Load the chunks (example)\n",
    "# loaded_docs = load_chunks()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
